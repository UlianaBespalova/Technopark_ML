{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "homework_4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKsjMeEQE8gj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f74a38ea-a99a-460e-880b-a1f7419f2b40"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcaLe5KPF_Eq"
      },
      "source": [
        "import pandas as pd\n",
        "import sys, re\n",
        "import numpy as np"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WX1xyFjrFz_a"
      },
      "source": [
        "train_pd = pd.read_csv('./drive/MyDrive/Colab_Notebooks/Technopark_ML/DZ_3/train.csv')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_7_Hoc5Gfjq"
      },
      "source": [
        "# (Токенизация/лемматизация на результат особо не повлияли)\n",
        "\n",
        "def clean_str(string):\n",
        "    string_cl = re.sub(r'[0-9-—–.,()@|\\/*«»:;_!?\\'\\\"№&#•⋆<>{}\\]\\[]', \"\", string)     \n",
        "    return string_cl.strip().lower() "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbwKVWkDh-hb"
      },
      "source": [
        "def clean_data(data, labels_flag=False):\n",
        "  data_norm = []\n",
        "  labels = []\n",
        "\n",
        "  for index, row in data.iterrows():\n",
        "    \n",
        "    data_norm.append(clean_str(row['title']))\n",
        "\n",
        "    if labels_flag==True:\n",
        "      if row['target']==True:\n",
        "        labels.append(1)\n",
        "      else:\n",
        "        labels.append(0)\n",
        "  return data_norm, labels    "
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxN17A-UDzXI"
      },
      "source": [
        "# ----------------------------------- Обучение -----------------------------------"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kGhfrbgoyge"
      },
      "source": [
        "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
        "\n",
        "    data = np.array(data)\n",
        "    data_size = len(data)\n",
        "    print(data_size)\n",
        "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
        "    for epoch in range(num_epochs):\n",
        "        if shuffle:\n",
        "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
        "            shuffled_data = data[shuffle_indices]\n",
        "        else:\n",
        "            shuffled_data = data\n",
        "        for batch_num in range(num_batches_per_epoch):\n",
        "            start_index = batch_num * batch_size\n",
        "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
        "            yield shuffled_data[start_index:end_index]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ko3D3AC6oyra"
      },
      "source": [
        "# Идея свистнута из статьи: http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "class TextCNN(object):\n",
        "    \"\"\"\n",
        "    A CNN for text classification.\n",
        "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "      self, sequence_length, num_classes, vocab_size,\n",
        "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
        "\n",
        "        # Placeholders for input, output and dropout\n",
        "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
        "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
        "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
        "\n",
        "        # Keeping track of l2 regularization loss (optional)\n",
        "        l2_loss = tf.constant(0.0)\n",
        "\n",
        "        # Embedding layer\n",
        "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
        "            self.W = tf.Variable(\n",
        "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
        "                name=\"W\")\n",
        "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
        "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
        "\n",
        "        # Create a convolution + maxpool layer for each filter size\n",
        "        pooled_outputs = []\n",
        "        for i, filter_size in enumerate(filter_sizes):\n",
        "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
        "                # Convolution Layer\n",
        "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
        "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
        "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
        "                conv = tf.nn.conv2d(\n",
        "                    self.embedded_chars_expanded,\n",
        "                    W,\n",
        "                    strides=[1, 1, 1, 1],\n",
        "                    padding=\"VALID\",\n",
        "                    name=\"conv\")\n",
        "                # Apply nonlinearity\n",
        "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
        "                # Maxpooling over the outputs\n",
        "                pooled = tf.nn.max_pool(\n",
        "                    h,\n",
        "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
        "                    strides=[1, 1, 1, 1],\n",
        "                    padding='VALID',\n",
        "                    name=\"pool\")\n",
        "                pooled_outputs.append(pooled)\n",
        "\n",
        "        # Combine all the pooled features\n",
        "        num_filters_total = num_filters * len(filter_sizes)\n",
        "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
        "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
        "\n",
        "        # Add dropout\n",
        "        with tf.name_scope(\"dropout\"):\n",
        "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
        "\n",
        "        # Final (unnormalized) scores and predictions\n",
        "        with tf.name_scope(\"output\"):\n",
        "            W = tf.get_variable(\n",
        "                \"W\",\n",
        "                shape=[num_filters_total, num_classes],\n",
        "                initializer=tf.contrib.layers.xavier_initializer())\n",
        "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
        "            l2_loss += tf.nn.l2_loss(W)\n",
        "            l2_loss += tf.nn.l2_loss(b)\n",
        "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
        "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
        "\n",
        "        # Calculate mean cross-entropy loss\n",
        "        with tf.name_scope(\"loss\"):\n",
        "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
        "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
        "\n",
        "        # Accuracy\n",
        "        with tf.name_scope(\"accuracy\"):\n",
        "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
        "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")     \n",
        "      "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZm_7d22glst"
      },
      "source": [
        "# Параметры данных\n",
        "dev_sample_percentage = 0.1\n",
        "\n",
        "# Параметры модели\n",
        "embedding_dim = 128\n",
        "filter_sizes = '3,4,5'\n",
        "num_filters=128\n",
        "dropout_keep_prob=0.5\n",
        "l2_reg_lambda = 0.0\n",
        "\n",
        "# Параметры обучения\n",
        "batch_size = 250 \n",
        "num_epochs = 100\n",
        "evaluate_every = 100\n",
        "checkpoint_every = 250\n",
        "num_checkpoints = 5\n",
        "\n",
        "allow_soft_placement = True\n",
        "log_device_placement = False\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxmD8eXxc1A8"
      },
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==1.13.2\n",
        "\n",
        "# (Пытаемся вытащить tensorflow.contrib)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkPAISVPgl46"
      },
      "source": [
        "from tensorflow.contrib import learn\n",
        "\n",
        "def preprocess(data_norm, labels):\n",
        "\n",
        "    print(\"Loading data...\")\n",
        "    x_text = data_norm\n",
        "    y = labels\n",
        "\n",
        "    # Собираем словарь\n",
        "    max_document_length = max([len(x.split()) for x in x_text])\n",
        "    print(\"len = \", max_document_length)\n",
        "\n",
        "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
        "    x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
        "\n",
        "    np.random.seed(42)\n",
        "    shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
        "    x_shuffled = np.array(x)[shuffle_indices]\n",
        "    y_shuffled = np.array(y)[shuffle_indices]\n",
        "\n",
        "\n",
        "    dev_sample_index = -1 * int(dev_sample_percentage * float(len(y)))\n",
        "    print(\"dev_sample_index = \", dev_sample_index)\n",
        "    x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
        "    y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
        "\n",
        "    del x, y, x_shuffled, y_shuffled\n",
        "\n",
        "    print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
        "    print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
        "    return x_train, y_train, vocab_processor, x_dev, y_dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2U5nZwci3es"
      },
      "source": [
        "data_norm, labels = clean_data(train_pd, labels_flag=True)\n",
        "x_train, y_train, vocab_processor, x_dev, y_dev = preprocess(data_norm, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIKxJ2-cEOes"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1LlR_mwgmDt"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "\n",
        "def train(x_train, y_train, vocab_processor, x_dev, y_dev):\n",
        "    # Training\n",
        "    # ==================================================\n",
        "    path = './drive/MyDrive/Colab_Notebooks/Technopark_ML/DZ_3/model'\n",
        "\n",
        "    with tf.Graph().as_default():\n",
        "        session_conf = tf.ConfigProto(\n",
        "          allow_soft_placement=allow_soft_placement,\n",
        "          log_device_placement=log_device_placement)\n",
        "        sess = tf.Session(config=session_conf)\n",
        "        with sess.as_default():\n",
        "            cnn = TextCNN(\n",
        "                sequence_length=x_train.shape[1],\n",
        "                num_classes=y_train.shape[1],\n",
        "                vocab_size=len(vocab_processor.vocabulary_),\n",
        "                embedding_size=embedding_dim,\n",
        "                filter_sizes=list(map(int, filter_sizes.split(\",\"))),\n",
        "                num_filters=num_filters,\n",
        "                l2_reg_lambda=l2_reg_lambda)\n",
        "\n",
        "            # Define Training procedure\n",
        "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "            optimizer = tf.train.AdamOptimizer(1e-3)\n",
        "            grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
        "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
        "\n",
        "            # Keep track of gradient values and sparsity (optional)\n",
        "            grad_summaries = []\n",
        "            for g, v in grads_and_vars:\n",
        "                if g is not None:\n",
        "                    grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
        "                    sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
        "                    grad_summaries.append(grad_hist_summary)\n",
        "                    grad_summaries.append(sparsity_summary)\n",
        "            grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
        "\n",
        "            # Output directory for models and summaries\n",
        "            timestamp = str(int(time.time()))\n",
        "            out_dir = os.path.abspath(os.path.join(path, \"runs\", timestamp))\n",
        "            print(\"Writing to {}\\n\".format(out_dir))\n",
        "\n",
        "            # Summaries for loss and accuracy\n",
        "            loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
        "            acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
        "\n",
        "            # Train Summaries\n",
        "            train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
        "            train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
        "            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
        "\n",
        "            # Dev summaries\n",
        "            dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
        "            dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
        "            dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
        "\n",
        "            # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
        "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
        "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
        "            if not os.path.exists(checkpoint_dir):\n",
        "                os.makedirs(checkpoint_dir)\n",
        "            saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n",
        "\n",
        "            # Write vocabulary\n",
        "            vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
        "\n",
        "            # Initialize all variables\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "\n",
        "            def train_step(x_batch, y_batch):\n",
        "                \"\"\"\n",
        "                A single training step\n",
        "                \"\"\"\n",
        "                feed_dict = {\n",
        "                  cnn.input_x: x_batch,\n",
        "                  cnn.input_y: y_batch,\n",
        "                  cnn.dropout_keep_prob: dropout_keep_prob\n",
        "                }\n",
        "                _, step, summaries, loss, accuracy = sess.run(\n",
        "                    [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
        "                    feed_dict)\n",
        "                time_str = datetime.datetime.now().isoformat()\n",
        "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
        "                train_summary_writer.add_summary(summaries, step)\n",
        "\n",
        "            def dev_step(x_batch, y_batch, writer=None):\n",
        "                \"\"\"\n",
        "                Evaluates model on a dev set\n",
        "                \"\"\"\n",
        "                feed_dict = {\n",
        "                  cnn.input_x: x_batch,\n",
        "                  cnn.input_y: y_batch,\n",
        "                  cnn.dropout_keep_prob: 1.0\n",
        "                }\n",
        "                step, summaries, loss, accuracy = sess.run(\n",
        "                    [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
        "                    feed_dict)\n",
        "                time_str = datetime.datetime.now().isoformat()\n",
        "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
        "                if writer:\n",
        "                    writer.add_summary(summaries, step)\n",
        "\n",
        "            # Generate batches\n",
        "            batches = batch_iter(\n",
        "                list(zip(x_train, y_train)), batch_size, num_epochs)\n",
        "            # Training loop. For each batch...\n",
        "            \n",
        "            for batch in batches: \n",
        "                # print(batch)\n",
        "                x_batch, y_batch = zip(*batch)\n",
        "                train_step(x_batch, y_batch)\n",
        "                current_step = tf.train.global_step(sess, global_step)\n",
        "                if current_step % evaluate_every == 0:\n",
        "                    print(\"\\nEvaluation:\")\n",
        "                    dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
        "                    print(\"\")\n",
        "                if current_step % checkpoint_every == 0:\n",
        "                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
        "                    print(\"Saved model checkpoint to {}\\n\".format(path))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFIFRRGvhw_V"
      },
      "source": [
        "# Обучение\n",
        "\n",
        "# train(x_train, y_train, vocab_processor, x_dev, y_dev)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-d0WofAmgdz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JgRmPIxEmhm"
      },
      "source": [
        "# -------------------------- тестирование на train-данных-------------------------------------"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ot_-8S81sNC"
      },
      "source": [
        "# Берем сохраненную обученную модель\n",
        "\n",
        "path = './drive/MyDrive/Colab_Notebooks/Technopark_ML/DZ_3/model'\n",
        "checkpoint_dir = './drive/MyDrive/Colab_Notebooks/Technopark_ML/DZ_3/model/runs/1636913449/checkpoints'\n",
        "\n",
        "vocab_path = os.path.join(checkpoint_dir, \"..\", \"vocab\")\n",
        "vocab_processor = learn.preprocessing.VocabularyProcessor.restore(vocab_path)\n",
        "\n",
        "checkpoint_file = tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUTFJRpm9ji8"
      },
      "source": [
        "def predict(data):\n",
        "\n",
        "  x_test = np.array(list(vocab_processor.transform(data)))\n",
        "  print(len(x_test))\n",
        "\n",
        "  graph = tf.Graph()\n",
        "  with graph.as_default():\n",
        "      session_conf = tf.ConfigProto(\n",
        "        allow_soft_placement=allow_soft_placement,\n",
        "        log_device_placement=log_device_placement)\n",
        "      sess = tf.Session(config=session_conf)\n",
        "      with sess.as_default():\n",
        "          # Грузим сохраненные переменные\n",
        "          saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
        "          saver.restore(sess, checkpoint_file)\n",
        "\n",
        "          input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
        "          dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
        "\n",
        "          predictions = graph.get_operation_by_name(\"output/predictions\").outputs[0]\n",
        "\n",
        "          batches = batch_iter(list(x_test), batch_size, 1, shuffle=False)\n",
        "\n",
        "          all_predictions = []\n",
        "\n",
        "          for x_test_batch in batches:\n",
        "              batch_predictions = sess.run(predictions, {input_x: x_test_batch, dropout_keep_prob: 1.0})\n",
        "              all_predictions = np.concatenate([all_predictions, batch_predictions])\n",
        "    \n",
        "  return all_predictions          "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zKoq6QPFLDc"
      },
      "source": [
        "data_norm, labels = clean_data(train_pd, labels_flag=True)\n",
        "all_predictions = predict(data_norm)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yz4bnZ2BFVg1",
        "outputId": "3c83cfef-6aa9-403a-f920-73fd81d22256"
      },
      "source": [
        "len(all_predictions)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "135309"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLgsnrnz9jwM",
        "outputId": "1eda70ae-5659-461b-ed69-4ef2c77d27ad"
      },
      "source": [
        "# Оценка результатов\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "if labels is not None:\n",
        "    correct_predictions = float(sum(all_predictions == labels))\n",
        "    print(\"Total number of test examples: {}\".format(len(labels)))\n",
        "    print(\"Accuracy: {:g}\".format(correct_predictions/float(len(labels))))\n",
        "\n",
        "    print(\"F1 = \", f1_score(labels, all_predictions))\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of test examples: 135309\n",
            "Accuracy: 0.996637\n",
            "F1 =  0.9863449476306233\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qakBURh-CWGD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qip4kDEXFjRr"
      },
      "source": [
        "# ------------------------Оценка данных из test.csv-----------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAofzserxZRO"
      },
      "source": [
        "test_pd = pd.read_csv('./drive/MyDrive/Colab_Notebooks/Technopark_ML/DZ_3/test.csv')"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynLyQuJMxZit",
        "outputId": "7df43da4-6f5f-473f-a01a-be7812daa0fd"
      },
      "source": [
        "len(test_pd)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "165378"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wT8tt80oxomh"
      },
      "source": [
        "x_raw_test, _ = clean_data(test_pd)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vf7-jFuKxo1p",
        "outputId": "8a957a6a-7a78-4600-92cd-6ffcc2dac352"
      },
      "source": [
        "len(x_raw_test)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "165378"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_K6nwQaQyy_z"
      },
      "source": [
        "all_pred_np = np.asarray(all_predictions)\n",
        "all_pred_bool = np.equal(all_pred_np, True)\n",
        "ids = test_pd['id']"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzGISmpyzk5f"
      },
      "source": [
        "ids = ids.to_frame()\n",
        "pred_ser = pd.Series(all_pred_bool)\n",
        "ids['label'] = pred_ser"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04mvX5dW3Qje"
      },
      "source": [
        "ids.to_csv(index=False, path_or_buf='./drive/MyDrive/Colab_Notebooks/Technopark_ML/DZ_3/res.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}